{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://nbviewer.jupyter.org/github/twMr7/PyTorch-Deep-Learning/blob/master/01-Basic_Tensors.ipynb\">\n",
    "        Open In Jupyter nbviewer\n",
    "        <img style=\"float: center;\" src=\"https://nbviewer.jupyter.org/static/img/nav_logo.svg\" width=\"120\" />\n",
    "    </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/twMr7/PyTorch-Deep-Learning/blob/master/01-Basic_Tensors.ipynb)\n",
    "\n",
    "# 1. Basic Tensors\n",
    "\n",
    "深度學習有許多有趣的應用，處理的資料不外乎文字、數字、影像、語音…等等的資料，但不管是什麼類型的資料，最終類神經網路由輸入到輸出的過程處理的還是數字，所以本質上都是在操作一堆數字的集合。 PyTorch 使用 *tensor* 作爲處理數字集合的基礎資料結構，與 *numpy ndarray* 非常類似，*tensor* 涵蓋了向量、矩陣、以及高維度陣列的概念，並且延伸加入了許多適用於深度學習的能力，例如最佳化的自動微分運算、在GPU上加速運算、分散在不同CPU、GPU、TPU裝置節點上的平行運算等。\n",
    "\n",
    "\n",
    "+ [**1.1 建構 Tensor**](#construct-tensor)\n",
    "+ [**1.2 Tensor 的屬性與基本操作**](#tensor-attributes)\n",
    "+ [**1.3 Tensor 的數學運算**](#math-ops)\n",
    "+ [**參考資料**](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"construct-tensor\"></a>\n",
    "\n",
    "## 1.1 建構 Tensor\n",
    "\n",
    "Tensor 可以透過幾種不同的方式建構出來（[torch API - Creation Ops](https://pytorch.org/docs/stable/torch.html#creation-ops)）：\n",
    "- 由現成的 *array_like* 的資料結構，如 `list` 或 `numpy.array`。\n",
    "- 使用內建函式，由特定數值模式安排建構新的 tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § 由 *list* 建構\n",
    "\n",
    "- [**`torch.tensor()`**](https://pytorch.org/docs/stable/generated/torch.tensor.html) - 會進行資料複製的操作。\n",
    "- [**`torch.as_tensor()`**](https://pytorch.org/docs/stable/generated/torch.as_tensor.html) - 資料是 *ndarray* 時儘可能不進行複製。\n",
    "\n",
    "未明確指定 *dtype* 的話，資料型態會從元素中自動推測。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) \n",
      "data type is torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 一層的 list\n",
    "list1 = [1, 2, 3]\n",
    "\n",
    "# 使用 tensor()\n",
    "tensor_list1 = torch.tensor(list1)\n",
    "\n",
    "print(tensor_list1, '\\ndata type is', tensor_list1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) \n",
      "data type is torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 兩層的 list，注意其中的元素同時有浮點數和整數\n",
    "list2 = [[1., 2., 3.], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# 使用 as_tensor()\n",
    "tensor_list2 = torch.as_tensor(list2)\n",
    "\n",
    "print(tensor_list2, '\\ndata type is', tensor_list2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § 由 *numpy ndarray* 建構\n",
    "\n",
    "*NumPy* 畢竟還是目前 Python 界最受歡迎的多維陣列函式庫，幾乎已經是許多機器學習和科學計算函式庫的共通標準了，PyTorch 對 *tensor* 的設計自然保留了許多可以相容於 *numpy ndarray* 的介面。\n",
    "\n",
    "- [**`torch.from_numpy()`**](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - 明確不複製，與 ndarray 共享記憶體。\n",
    "- [**`torch.Tensor.numpy()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.numpy) - 將 tensor 轉成 numpy ndarray。\n",
    "\n",
    "`torch.tensor()`以及`torch.as_tensor()`一樣可以用 *ndarray* 建構 *tensor* ，但爲了避免“複製”這種昂貴的操作，建議 *ndarray* 指定使用`torch.from_numpy()`。 當 *tensor* 與 *ndarray* 共用同一份底層的記憶體時，改變 *tensor* 的內容也會同時變更 *ndarray* 的內容；同理，改變 *ndarray* 元素一樣會變更 *tensor* 的元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 二維矩陣的 array \n",
    "array2 = np.array(list2)\n",
    "\n",
    "# 使用 from_numpy() 生成 tensor\n",
    "tensor_array2 = torch.from_numpy(array2)\n",
    "\n",
    "print(tensor_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor to array:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "-- array2:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# 將 tensor 轉爲 numpy array\n",
    "t2a = tensor_array2.numpy()\n",
    "\n",
    "print('-- tensor to array:\\n', t2a)\n",
    "\n",
    "# 內容與原始的 array 一樣\n",
    "print('\\n-- array2:\\n', array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor to array:\n",
      " [[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8. 10.]]\n",
      "\n",
      "-- tensor_array2:\n",
      " tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8., 10.]], dtype=torch.float64)\n",
      "\n",
      "-- array2:\n",
      " [[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# 底層記憶體其實是共用的\n",
    "t2a[-1, -1] = 10\n",
    "\n",
    "print('-- tensor to array:\\n', t2a)\n",
    "\n",
    "# 所有共用記憶體的內容都一起改變了\n",
    "print('\\n-- tensor_array2:\\n', tensor_array2)\n",
    "print('\\n-- array2:\\n', array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § 由內建函式建構新的 Tensor\n",
    "\n",
    "內建用來建構 *tensor* 的函式，有很多名字刻意設計與 *numpy* 的一樣，讓原本熟悉 *numpy* 的開發者可以很快上手。\n",
    "- [**`torch.zeros()`**](https://pytorch.org/docs/stable/generated/torch.zeros.html) - 生成元素都是 **0** 的 *tensor*。\n",
    "- [**`torch.ones()`**](https://pytorch.org/docs/stable/generated/torch.ones.html) - 生成元素都是 **1** 的 *tensor*。\n",
    "- [**`torch.zeros_like()`**](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) - 比照輸入物件的維度及大小，生成元素都是 **0** 的 *tensor*。\n",
    "- [**`torch.ones_like()`**](https://pytorch.org/docs/stable/generated/torch.ones_like.html) - 比照輸入物件的維度及大小，生成元素都是 **1** 的 *tensor*。\n",
    "- [**`torch.arange()`**](https://pytorch.org/docs/stable/generated/torch.arange.html) - 用指定範圍的數列（通常是整數）產生一維 *tensor*。\n",
    "- [**`torch.linspace()`**](https://pytorch.org/docs/stable/generated/torch.arange.html) - 在指定上下限數字間（通常是浮點數），以均勻間隔的數列產生一維 *tensor*。\n",
    "- [**`torch.rand()`**](https://pytorch.org/docs/stable/generated/torch.rand.html) - 從 *uniform* 分佈裡產生 $[0,1)$ 之間指定維度及大小的隨機數 *tensor*。\n",
    "- [**`torch.randn()`**](https://pytorch.org/docs/stable/generated/torch.randn.html) - 從 *standard normal* 分佈（平均爲0且變異數爲1）裡產生之間指定維度及大小的隨機數 *tensor*。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor zero2:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 比照 tensor_array2 的形狀，產生一個新的元素都爲0的 tensor\n",
    "tensor_zero2 = torch.zeros_like(tensor_array2)\n",
    "\n",
    "print('-- tensor zero2:\\n', tensor_zero2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor 5x5 randn:\n",
      " tensor([[ 0.1132,  0.1963,  0.6799, -1.6679,  1.2001],\n",
      "        [ 0.4269,  0.7143, -0.4991,  0.9837, -1.3789],\n",
      "        [ 0.8069, -1.1253, -1.0976, -0.7707,  0.1946],\n",
      "        [-0.8130,  2.3870, -0.2506, -0.3445, -0.7529],\n",
      "        [-0.5718, -1.5279,  0.9815,  1.3052,  0.6815]])\n"
     ]
    }
   ],
   "source": [
    "# 指定 5x5 的尺寸，產生標準常態分佈隨機數\n",
    "tensor_randn = torch.randn((5,5))\n",
    "\n",
    "print('-- tensor 5x5 randn:\\n', tensor_randn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tensor-attributes\"></a>\n",
    "\n",
    "## 1.2 Tensor 的屬性與基本操作\n",
    "\n",
    "[**Tensor 的屬性**](https://pytorch.org/docs/stable/tensor_attributes.html)\n",
    "\n",
    "每一個 *tensor* 物件都包含了描述 *tensor* 的形狀、大小、資料型態等屬性。\n",
    "- [**`torch.dtype`**](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype) - 資料型態。\n",
    "- [**`torch.shape`**](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype) - 維度形狀，`size()`的別名。\n",
    "- [**`torch.size()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size) - 維度形狀。\n",
    "- [**`torch.dim()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.dim) - 維度。\n",
    "- [**`torch.numel()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size) - 元素個數。\n",
    "- [**`torch.device`**](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) - 資料配置所在的裝置。\n",
    "- [**`torch.Tensor.stride()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.stride) - 每個維度成員之間的步幅、間距。\n",
    "- [**`torch.Tensor.storage()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.storage) - 底層的儲存物件。\n",
    "- [**`torch.Tensor.storage_type()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.storage_type) - 底層的儲存型態。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float32\n",
      "shape: torch.Size([5, 5])\n",
      "size: torch.Size([5, 5])\n",
      "numel: 25\n",
      "device: cpu\n",
      "stride: (5, 1)\n",
      "storage type: <class 'torch.FloatStorage'>\n",
      "storage:  0.11315546929836273\n",
      " 0.1962723433971405\n",
      " 0.6798641681671143\n",
      " -1.6679118871688843\n",
      " 1.2000519037246704\n",
      " 0.426929771900177\n",
      " 0.7142747640609741\n",
      " -0.49911901354789734\n",
      " 0.9836630821228027\n",
      " -1.3789368867874146\n",
      " 0.8068822622299194\n",
      " -1.1252902746200562\n",
      " -1.0975691080093384\n",
      " -0.7707319259643555\n",
      " 0.1946474015712738\n",
      " -0.8129519820213318\n",
      " 2.3870155811309814\n",
      " -0.25057148933410645\n",
      " -0.3444853723049164\n",
      " -0.7529102563858032\n",
      " -0.5718392729759216\n",
      " -1.5279439687728882\n",
      " 0.9815207719802856\n",
      " 1.305169939994812\n",
      " 0.6814917325973511\n",
      "[torch.FloatStorage of size 25]\n"
     ]
    }
   ],
   "source": [
    "print('dtype:', tensor_randn.dtype)\n",
    "print('shape:', tensor_randn.shape)\n",
    "print('size:', tensor_randn.size())\n",
    "print('numel:', tensor_randn.numel())\n",
    "print('device:', tensor_randn.device)\n",
    "print('stride:', tensor_randn.stride())\n",
    "print('storage type:', tensor_randn.storage_type())\n",
    "print('storage:', tensor_randn.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § Tensor 的資料型態及轉換方法\n",
    "\n",
    "建構 *tensor* 的函式都可以直接指定配置那一種資料型態，如 `torch.float`、`torch.double`、`torch.int`、 ...等等，或是在需要時進行轉換。\n",
    "\n",
    "- [**`tensor.to()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) - 轉換資料型態或暫存記憶體裝置的通用函式。\n",
    "- [**`tensor.double()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.double) - 轉換資料型態爲 *double* (64-bit 浮點數)。\n",
    "- [**`tensor.float()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.float) - 轉換資料型態爲 *float* (32-bit 浮點數)。\n",
    "- [**`tensor.half()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.half) - 轉換資料型態爲 *half* (16-bit 浮點數)。\n",
    "- [**`tensor.int()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.int) - 轉換資料型態爲 *int* (32-bit 整數)。\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor arange:\n",
      " tensor([2, 3, 4, 5, 6]) , dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 建構指定範圍的數列\n",
    "tensor_arange = torch.arange(2, 7)\n",
    "\n",
    "print('-- tensor arange:\\n', tensor_arange, ', dtype:', tensor_arange.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor halfarange:\n",
      " tensor([2., 3., 4., 5., 6.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 轉換資料型態，使用 half()\n",
    "tensor_halfarange = tensor_arange.half()\n",
    "\n",
    "print('-- tensor halfarange:\\n', tensor_halfarange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor floatarange:\n",
      " tensor([2., 3., 4., 5., 6.]) , dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 轉換資料型態，使用 to()\n",
    "tensor_floatarange = tensor_halfarange.to(torch.float32)\n",
    "\n",
    "print('-- tensor floatarange:\\n', tensor_floatarange, ', dtype:', tensor_floatarange.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § Tensor 的計算裝置\n",
    "\n",
    "深度學習經常需要藉由 GPU 加快訓練或推論的速度，因此 *tensor* 帶有計算的記憶體裝置屬性，可以視需求指定在 CPU 或 GPU 上計算。 *tensor* 的建構函式都有提供 *device* 的參數，未指定的話預設裝置是 CPU，`tensor.to()` 函式可以用來將資料複製到指定裝置。\n",
    "\n",
    "- [**`tensor.to()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) - 轉換資料型態或儲存裝置的通用函式，*device* 參數可以是字串 `'cuda'` 或 `'cpu'`。\n",
    "- [**`tensor.cuda()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda) - 將 *tensor* 資料複製到 GPU。\n",
    "- [**`tensor.cpu()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu) - 將 *tensor* 資料複製到 CPU。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor 3x4 rand:\n",
      "tensor([[0.8619, 0.3442, 0.2436, 0.6928],\n",
      "        [0.0634, 0.1429, 0.0647, 0.5768],\n",
      "        [0.3780, 0.8026, 0.2909, 0.3807]])\n",
      "Device: cpu\n",
      "Storage type: <class 'torch.FloatStorage'>\n"
     ]
    }
   ],
   "source": [
    "# 生成 3x4 均勻分佈的隨機數 tensor\n",
    "tensor_rand = torch.rand((3,4))\n",
    "\n",
    "print('-- tensor 3x4 rand:\\n{}\\nDevice: {}\\nStorage type: {}'.format(\n",
    "    tensor_rand, tensor_rand.device, tensor_rand.storage_type()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tensor rand on GPU:\n",
      "tensor([[0.8619, 0.3442, 0.2436, 0.6928],\n",
      "        [0.0634, 0.1429, 0.0647, 0.5768],\n",
      "        [0.3780, 0.8026, 0.2909, 0.3807]], device='cuda:0')\n",
      "Device: cuda:0\n",
      "Storage type: <class 'torch.cuda.FloatStorage'>\n"
     ]
    }
   ],
   "source": [
    "# 檢查 GPU 裝置是否存在\n",
    "if torch.cuda.is_available():\n",
    "    tensor_randgpu = tensor_rand.to('cuda')\n",
    "    print('-- tensor rand on GPU:\\n{}\\nDevice: {}\\nStorage type: {}'.format(\n",
    "        tensor_randgpu, tensor_randgpu.device, tensor_randgpu.storage_type()))\n",
    "else:\n",
    "    print('GPU device is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § Tensor 的索引方法\n",
    "\n",
    "*Tensor* 的索引方法與 *numpy ndarray* 以及 python 內建的 *list* 相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8., 10.]], dtype=torch.float64)\n",
      "tensor([ 7.,  8., 10.], dtype=torch.float64)\n",
      "tensor([[4., 5., 6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(tensor_array2[:])\n",
    "print(tensor_array2[-1])\n",
    "print(tensor_array2[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.], dtype=torch.float64)\n",
      "tensor([3., 6.], dtype=torch.float64)\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(tensor_array2[1, :])\n",
    "print(tensor_array2[0:2, 2])\n",
    "print(tensor_array2[:-1, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### § Tensor 的串接分割與維度調整\n",
    "\n",
    "- [**`torch.cat()`**](https://pytorch.org/docs/stable/generated/torch.cat.html) - 按照指定的維度方向串接多個 *tensor*。\n",
    "- [**`torch.split()`**](https://pytorch.org/docs/stable/generated/torch.split.html) - 將 *tensor* 按照指定的方式切割成小塊。\n",
    "- [**`torch.reshape()`**](https://pytorch.org/docs/stable/generated/torch.reshape.html) - 將 *tensor* 調整成指定的維度形狀，若形狀相容返回 *view*，否則會複製。\n",
    "- [**`torch.transpose()`**](https://pytorch.org/docs/stable/generated/torch.transpose.html) - 轉置 *tensor* 指定的維度。\n",
    "- [**`torch.t()`**](https://pytorch.org/docs/stable/generated/torch.t.html) - 一般二維矩陣的轉置，等同 `transpose(input, 0, 1)`。\n",
    "- ...\n",
    "\n",
    "許多 *tensor* 的操作支援共享現有記憶體中數據的 [**view**](https://pytorch.org/docs/stable/tensor_view.html)，這些操作避免了記憶體複製，可以更快更有效率地存取片段、調整形狀等。\n",
    "\n",
    "- [**`torch.Tensor.view()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) - 返回指定的維度形狀的 *view* 。\n",
    "- [**`torch.Tensor.squeeze()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze) - 返回沒有任何維度是 1 的 *view*。\n",
    "- [**`torch.Tensor.unsqueeze()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze) - 返回插入 1 個指定維度的 *view*。\n",
    "- [**`torch.Tensor.split()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.split) - 返回指定方式切割成小塊的 *view*。\n",
    "- [**`torch.Tensor.transpose()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.transpose) - 返回指定維度置換後的 *view*。\n",
    "- [**`torch.Tensor.t()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t) - 一般二維矩陣的轉置，等同 `transpose(input, 0, 1)`。\n",
    "- [**`torch.Tensor.T`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t) - 返回所有維度反向的 *view*，如果是二維矩陣，效果與 `torch.Tensor.t()` 相同。\n",
    "- ...\n",
    "\n",
    "***Note***: 有些操作是定義在 *torch* 的方法，如 **`torch.cat()`**；有些是定義在 *torch.Tensor* 的方法，如 **`torch.Tensor.view()`**；有些則是呼叫在 *torch* 定義的或是 *torch.Tensor* 的都一樣，例如 **`torch.split()`** 與 **`torch.Tensor.split()`**。\n",
    "\n",
    "***Note***: 看到 **torch.Tensor** 的方法名字後面有底線的，就是對應相同方法的 in-place（就地變更）版本。 如 `torch.Tensor.transpose_()` 就是對應 `torch.Tensor.transpose()` 的就地變更版本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sequence of tensors concatenated, shape=torch.Size([3, 10]):\n",
      "tensor([[ 1.0000,  2.0000,  3.0000,  0.0000,  0.0000,  0.0000,  0.7083,  0.5107,\n",
      "          0.0899,  0.2314],\n",
      "        [ 4.0000,  5.0000,  6.0000,  0.0000,  0.0000,  0.0000,  0.3705,  0.1365,\n",
      "          0.1549,  0.0774],\n",
      "        [ 7.0000,  8.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.7773,  0.7199,\n",
      "          0.4946,  0.8288]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# concatenate a sequence of tensors\n",
    "wide_cat = torch.cat([tensor_array2, tensor_zero2, tensor_rand], dim=1)\n",
    "\n",
    "print('A sequence of tensors concatenated, shape={}:\\n{}'.format(wide_cat.shape, wide_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A long view from wide:\n",
      " tensor([[ 1.0000,  2.0000],\n",
      "        [ 3.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.7083,  0.5107],\n",
      "        [ 0.0899,  0.2314],\n",
      "        [ 4.0000,  5.0000],\n",
      "        [ 6.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.3705,  0.1365],\n",
      "        [ 0.1549,  0.0774],\n",
      "        [ 7.0000,  8.0000],\n",
      "        [10.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.7773,  0.7199],\n",
      "        [ 0.4946,  0.8288]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# view with a different shape\n",
    "long_view = wide_cat.view(15, -1)\n",
    "\n",
    "print('A long view from wide:\\n', long_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 底層儲存實際數據的記憶體是一樣的位置\n",
    "long_view.storage().data_ptr() == wide_cat.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_view stride = (2, 1)\n",
      "wide_cat stride = (10, 1)\n"
     ]
    }
   ],
   "source": [
    "print('long_view stride = {}'.format(long_view.stride()))\n",
    "print('wide_cat stride = {}'.format(wide_cat.stride()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.7083, 0.5107],\n",
      "        [0.0899, 0.2314]], dtype=torch.float64)\n",
      "tensor([[4.0000, 5.0000],\n",
      "        [6.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.3705, 0.1365],\n",
      "        [0.1549, 0.0774]], dtype=torch.float64)\n",
      "tensor([[ 7.0000,  8.0000],\n",
      "        [10.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.7773,  0.7199],\n",
      "        [ 0.4946,  0.8288]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# column 方向分割固定長度\n",
    "for s in long_view.split(5, dim=0):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "tensor([[0.7083, 0.5107],\n",
      "        [0.0899, 0.2314],\n",
      "        [4.0000, 5.0000],\n",
      "        [6.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], dtype=torch.float64)\n",
      "tensor([[ 0.3705,  0.1365],\n",
      "        [ 0.1549,  0.0774],\n",
      "        [ 7.0000,  8.0000],\n",
      "        [10.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.7773,  0.7199],\n",
      "        [ 0.4946,  0.8288]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# column 方向分割指定長度\n",
    "for s in long_view.split([3, 5, 7], dim=0):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"math-ops\"></a>\n",
    "\n",
    "## 1.3 Tensor 的數學運算\n",
    "\n",
    "[**數學運算**](https://pytorch.org/docs/stable/torch.html#math-operations)\n",
    "\n",
    "算數運算子 \" $+\\,-\\,*\\,/$ \" 的操作與 *numpy* 有相同的效果，矩陣乘法運算子 $@$ 也一樣，運算的 *broadcast* 概念也相同。\n",
    "\n",
    "- [**`torch.square()`**](https://pytorch.org/docs/stable/generated/torch.square.html) - 平方。\n",
    "- [**`torch.exp()`**](https://pytorch.org/docs/stable/generated/torch.exp.html) - exponential。\n",
    "- [**`torch.log()`**](https://pytorch.org/docs/stable/generated/torch.log.html) - natural logarithm。\n",
    "- [**`torch.sum()`**](https://pytorch.org/docs/stable/generated/torch.sum.html) - 所有元素的加總。\n",
    "- [**`torch.mean()`**](https://pytorch.org/docs/stable/generated/torch.mean.html) - 所有元素的平均。\n",
    "- [**`torch.std()`**](https://pytorch.org/docs/stable/generated/torch.std.html) - 所有元素的標準差。\n",
    "- ...\n",
    "\n",
    "運算結果若是單一元素的純量，仍然會是返回 *tensor* 型態的資料，可以透過 [**`torch.Tensor.item()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item) 取出標準的 python 純量數值型別。 否則，多個元素的 *tensor* 也可以透過 [**`torch.Tensor.tolist()`**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.tolist) 轉成 python 的 *list* 型別資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8., 10.]], dtype=torch.float64)\n",
      "tensor([[0.8619, 0.3442, 0.2436, 0.6928],\n",
      "        [0.0634, 0.1429, 0.0647, 0.5768],\n",
      "        [0.3780, 0.8026, 0.2909, 0.3807]])\n"
     ]
    }
   ],
   "source": [
    "# 現有變數\n",
    "print(tensor_array2)\n",
    "print(tensor_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.],\n",
       "        [ 49.,  64., 100.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise 一對一相乘\n",
    "tensor_array2 * tensor_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.],\n",
       "        [ 49.,  64., 100.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise 平方\n",
    "torch.square(tensor_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8619,  4.0634,  6.3780],\n",
       "        [ 8.8619, 10.0634, 12.3780],\n",
       "        [14.8619, 16.0634, 20.3780]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乘法、加法、與 broadcast，使用算數運算子\n",
    "tensor_array2 * 2 + tensor_rand[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8619,  4.0634,  6.3780],\n",
       "        [ 8.8619, 10.0634, 12.3780],\n",
       "        [14.8619, 16.0634, 20.3780]], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乘法、加法、與 broadcast，使用函式呼叫\n",
    "torch.add(torch.multiply(tensor_array2, 2), tensor_rand[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14.,  32.,  53.],\n",
       "        [ 32.,  77., 128.],\n",
       "        [ 53., 128., 213.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩陣乘法，使用算數運算子\n",
    "tensor_array2 @ tensor_array2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14.,  32.,  53.],\n",
       "        [ 32.,  77., 128.],\n",
       "        [ 53., 128., 213.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩陣乘法，使用函式呼叫\n",
    "torch.matmul(tensor_array2, tensor_array2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4036)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算結果為單一元素\n",
    "torch.sum(tensor_rand) / tensor_rand.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4035583436489105"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 單一元素取 python 數值\n",
    "torch.mean(tensor_rand).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "\n",
    "## References:\n",
    "+ PyTorch Documentation [[link]](https://pytorch.org/docs/)\n",
    "+ Eli Stevens, Luca Antiga, and Thomas Viehmann. \"*Deep Learning with PyTorch*\". Manning Publications Co, 2020. [[link]](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)\n",
    "+ Suraj Subramanian, et al. \"*Learn the Basics - Tensors*\". PyTorch.org. [[link]](https://pytorch.org/tutorials/beginner/basics/tensor_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**May, 2021**\n",
    "\n",
    "***James Chang***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
